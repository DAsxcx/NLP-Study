{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4839ae78-f798-4c99-a27c-0b1a49adbae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 14:38:17,120: INFO: running /root/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py-f/root/.local/share/jupyter/runtime/kernel-f2fbbaf0-d4dc-499e-b3d9-1eb7e09bbe5c.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c7197d67034088a9dcc1e8f006b353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6197ddb3d443a5b18a96c3b42ee02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6da8db7f8d54e539bcca4e5e915ecfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,721,666 || all params: 1,571,635,204 || trainable%: 0.3004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_1183/374155221.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 1:22:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.691430</td>\n",
       "      <td>0.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.688448</td>\n",
       "      <td>0.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.687544</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 16:22:38,983: INFO: result saved!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import sys\n",
    "import logging\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, DebertaV2Tokenizer, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train = pd.read_csv(\"./imdb/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"./imdb/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    program = os.path.basename(sys.argv[0])\n",
    "    logger = logging.getLogger(program)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    logger.info(r\"running %s\" % ''.join(sys.argv))\n",
    "\n",
    "    train, val = train_test_split(train, test_size=.2)\n",
    "\n",
    "    train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\n",
    "    val_dict = {'label': val[\"sentiment\"], 'text': val['review']}\n",
    "    test_dict = {\"text\": test['review']}\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_dict)\n",
    "    val_dataset = datasets.Dataset.from_dict(val_dict)\n",
    "    test_dataset = datasets.Dataset.from_dict(test_dict)\n",
    "\n",
    "    #batch_size = 32\n",
    "    batch_size = 4\n",
    "\n",
    "    model_id = \"microsoft/deberta-v2-xxlarge\"\n",
    "\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        use_safetensors=True\n",
    "        # device_map=\"auto\",\n",
    "        # load_in_8bit=True\n",
    "    )\n",
    "\n",
    "    # Define LoRA Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        # target_modules=['q_proj', 'v_proj'],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    # model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    # add LoRA adaptor\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./checkpoint',  # output directory\n",
    "        num_train_epochs=3,  # total number of training epochs\n",
    "        per_device_train_batch_size=2,  # batch size per device during training\n",
    "        per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./logs',  # directory for storing logs\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        #evaluation_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=tokenized_train,  # training dataset\n",
    "        eval_dataset=tokenized_val,  # evaluation dataset\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    prediction_outputs = trainer.predict(tokenized_test)\n",
    "    test_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\n",
    "    print(test_pred)\n",
    "\n",
    "    result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\n",
    "    result_output.to_csv(\"./result/deberta_lora_int8.csv\", index=False, quoting=3)\n",
    "    logging.info('result saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
