# Prompt Tuning

## 什么是 Prompt Tuning？

Prompt Tuning（提示调优）是一种**参数高效**的微调技术。它不修改模型的大量内部参数，  
而是在输入端添加或优化一小段**可学习的提示（prompt）**，引导预训练模型完成下游任务。

- **核心思路**：只学习“提示”，不动模型主体  
- **优点**：训练速度快、资源消耗低、更易扩展到新任务  
- **适用模型**：GPT、BERT、RoBERTa、T5 等各种大型预训练模型

---

## 为什么要学 Prompt Tuning？

- **节省算力 & 存储**  
  只需优化几百到几千个参数（提示向量），而非模型数亿甚至数十亿参数。  
- **极速迭代**  
  加载大模型后，几个 epoch 就能看到效果；无需漫长的全量微调。  
- **跨任务复用**  
  同一个大模型，通过不同的 prompt tuning，可适配文本分类、问答、生成、翻译等多种任务。

---

## 硬提示（Hard Prompt） vs. 软提示（Soft Prompt）

| 特性              | 硬提示（Hard Prompt）                      | 软提示（Soft Prompt）                        |
| ----------------- | ------------------------------------------ | -------------------------------------------- |
| 表示形式          | 人工设计的一段固定文本                      | 一组可训练的向量（虚拟 token embeddings）     |
| 可训练性          | 不可训练                                  | 可训练                                       |
| 灵活性            | 句式固定，需手动调整                        | 自动学习，适应性更强                           |
| 示例              | `"评论：{text} 情感是[MASK]"`               | `[V1][V2] ... [Vk] {text}` （V1–Vk 为可训练向量） |

---

## Prompt Tuning 的核心原理

1. **引入虚拟 Token**  
   在输入序列前或中间插入 `k` 个“虚拟 token”（soft prompt），它们的向量是可训练参数。  
2. **冻结主体模型**  
   除提示向量外，保持预训练模型参数不变。  
3. **梯度更新**  
   仅对提示向量进行反向传播、梯度下降，学习一组最优提示。  
4. **推理阶段**  
   将学习好的提示向量与真实输入拼接，送入模型，输出下游任务预测结果。

---
## prompt技术对比
| 特性              | Prompt Tuning                             | Prefix Tuning                              |
|-------------------|-------------------------------------------|--------------------------------------------|
| 插入位置          | 输入序列前添加可训练的虚拟 token          | 每层 transformer 的 key/value 前加前缀     |
| 参数规模          | 极小，仅需少量可训练 embedding             | 较小，但比 Prompt Tuning 多                |
| 适用模型          | 一般适用于 encoder-only 或 decoder-only   | 多用于 decoder-only 或 encoder-decoder     |
| 表现能力          | 简单任务有效，复杂任务时可能不足           | 更强，适用于复杂下游任务                   |


### 参考资料
[Prompt Tuning解读]("https://zhuanlan.zhihu.com/p/618871247")

[The Power of Scale for Parameter-Efficient Prompt Tuning]("https://arxiv.org/abs/2104.08691")
